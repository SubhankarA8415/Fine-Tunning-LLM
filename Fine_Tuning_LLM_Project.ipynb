{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3037b264c97643e1bff5bf511c60db2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6940c6bd48b94a5e8351732eecadb7c9",
              "IPY_MODEL_ac5a00aff87646fa86fb684980b2387d",
              "IPY_MODEL_e9ebed42158845d79c861f26df1b1743"
            ],
            "layout": "IPY_MODEL_b8d56d0a87ac477788edb424149b26d0"
          }
        },
        "6940c6bd48b94a5e8351732eecadb7c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_179f0a461f6c4a569b1008a7ac2fb551",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5bb1846e8efa4174832071e18788f707",
            "value": "Map:‚Äá100%"
          }
        },
        "ac5a00aff87646fa86fb684980b2387d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05a2b23b09d641a29162a4d17a540061",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e44070ef8a143a28581b27d8f7e6e58",
            "value": 60
          }
        },
        "e9ebed42158845d79c861f26df1b1743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d37b526e2d43a29e0ce064ffd8e617",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_67d71bf324be47c6af9f2c1ada75ff87",
            "value": "‚Äá60/60‚Äá[00:00&lt;00:00,‚Äá946.49‚Äáexamples/s]"
          }
        },
        "b8d56d0a87ac477788edb424149b26d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "179f0a461f6c4a569b1008a7ac2fb551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb1846e8efa4174832071e18788f707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05a2b23b09d641a29162a4d17a540061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e44070ef8a143a28581b27d8f7e6e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06d37b526e2d43a29e0ce064ffd8e617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67d71bf324be47c6af9f2c1ada75ff87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#üìò Fine-Tuning a Large Language Model\n",
        "\n",
        "**Objective:**  \n",
        "This project demonstrates how a pretrained **Large Language Model (LLM)** can be fine-tuned using a **custom, textbook-derived dataset**.\n",
        "\n",
        "**Key Focus Areas:**\n",
        "- üìÇ Dataset preparation in **JSONL format**\n",
        "- üîÅ Supervised fine-tuning\n",
        "- üìâ Observing training loss\n",
        "- üéì Academic domain adaptation\n",
        "\n",
        "> The dataset is written in a **book-friendly academic style** based on the prescribed LLM textbook.\n"
      ],
      "metadata": {
        "id": "M5Y243ddysgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Please ensure that the runtime is connected with Tesla-T4 GPU not normal CPU, It is important for training the model."
      ],
      "metadata": {
        "id": "X9d4hLWDZhdk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKsfffzKrGdK",
        "outputId": "2457eaf4-1d66-44cf-9393-e1277c3bd3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch accelerate matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Importing Required Libraries\n",
        "\n",
        "The following libraries are used in this project:\n",
        "\n",
        "- **transformers** ‚Üí Pretrained models and training utilities  \n",
        "- **datasets** ‚Üí Dataset loading and processing  \n",
        "- **torch** ‚Üí Deep learning backend  \n",
        "\n",
        "These tools enable efficient fine-tuning of transformer-based models.\n"
      ],
      "metadata": {
        "id": "D-lQ2QHM0HFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "exGIpvuq0WaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Dataset Overview\n",
        "\n",
        "- **Format:** JSON Lines (**JSONL**)  \n",
        "- **Total Samples:** 60  \n",
        "- **Domain:** Large Language Models  \n",
        "- **Style:** Academic, book-friendly  \n",
        "\n",
        "Each dataset entry contains:\n",
        "- A **question** related to LLM concepts  \n",
        "- A concise **explanatory answer**\n"
      ],
      "metadata": {
        "id": "FWKLdOWAs39j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before running the cell below, please ensure that training_data_small.jsonl has been imported in the local files of colab after connecting the runtime, otherwise it will result in error."
      ],
      "metadata": {
        "id": "qvySDjdDY_Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data_list = []\n",
        "\n",
        "with open(\"training_data_small.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        data_list.append({\n",
        "            \"question\": obj[\"messages\"][0][\"content\"],\n",
        "            \"answer\": obj[\"messages\"][1][\"content\"]\n",
        "        })\n",
        "\n",
        "# Dataset statistics\n",
        "print(\"Data type:\", type(data_list))\n",
        "print(\"Total samples:\", len(data_list))\n",
        "\n",
        "# Display one sample question‚Äìanswer pair\n",
        "print(\"\\nSample Question:\")\n",
        "print(data_list[0][\"question\"])\n",
        "\n",
        "print(\"\\nSample Answer:\")\n",
        "print(data_list[0][\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXh6JvDqtACe",
        "outputId": "0a87aa09-b2d5-4729-ea88-8b861e1955f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: <class 'list'>\n",
            "Total samples: 60\n",
            "\n",
            "Sample Question:\n",
            "What is a Large Language Model?\n",
            "\n",
            "Sample Answer:\n",
            "A Large Language Model is a transformer-based neural network trained on large text corpora to understand and generate natural language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Model Selection\n",
        "\n",
        "A lightweight pretrained transformer model is chosen to:\n",
        "\n",
        "- ‚ö° Train efficiently on Google Colab  \n",
        "- üíæ Reduce memory and compute requirements  \n",
        "- üéØ Clearly demonstrate fine-tuning concepts  \n",
        "\n",
        "> The focus is on **understanding the fine-tuning workflow**, not achieving state-of-the-art performance.\n"
      ],
      "metadata": {
        "id": "LeSt3jhes8fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilgpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "7l4GxrqytSJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2a64c4-f64d-4a4f-9bd1-4688c71b62b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RYOmnQHCwdjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_list(data_list)\n",
        "dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy1jPYsEwfoe",
        "outputId": "0c48b086-c680-4f56-cccc-9ec5a206c290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer'],\n",
              "    num_rows: 60\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è Tokenization Strategy\n",
        "\n",
        "The **question and answer** are combined into a single training sequence:\n",
        "```\n",
        "Question: <question text>\n",
        "Answer: <answer text>\n",
        "```\n",
        "This allows the model to **learn how to generate academic answers** given a question.\n"
      ],
      "metadata": {
        "id": "WCdKNrBdtEba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    text = f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function)\n"
      ],
      "metadata": {
        "id": "g8yPWr9PtD_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3037b264c97643e1bff5bf511c60db2a",
            "6940c6bd48b94a5e8351732eecadb7c9",
            "ac5a00aff87646fa86fb684980b2387d",
            "e9ebed42158845d79c861f26df1b1743",
            "b8d56d0a87ac477788edb424149b26d0",
            "179f0a461f6c4a569b1008a7ac2fb551",
            "5bb1846e8efa4174832071e18788f707",
            "05a2b23b09d641a29162a4d17a540061",
            "5e44070ef8a143a28581b27d8f7e6e58",
            "06d37b526e2d43a29e0ce064ffd8e617",
            "67d71bf324be47c6af9f2c1ada75ff87"
          ]
        },
        "outputId": "5ca41a9f-003f-4002-e20e-9c6180575fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3037b264c97643e1bff5bf511c60db2a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenized dataset structure\n",
        "tokenized_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFs20LGlyyX0",
        "outputId": "3cfa56f3-5774-48e5-a995-2cf82a518720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is a Large Language Model?',\n",
              " 'answer': 'A Large Language Model is a transformer-based neural network trained on large text corpora to understand and generate natural language.',\n",
              " 'input_ids': [24361,\n",
              "  25,\n",
              "  1867,\n",
              "  318,\n",
              "  257,\n",
              "  13601,\n",
              "  15417,\n",
              "  9104,\n",
              "  30,\n",
              "  198,\n",
              "  33706,\n",
              "  25,\n",
              "  317,\n",
              "  13601,\n",
              "  15417,\n",
              "  9104,\n",
              "  318,\n",
              "  257,\n",
              "  47385,\n",
              "  12,\n",
              "  3106,\n",
              "  17019,\n",
              "  3127,\n",
              "  8776,\n",
              "  319,\n",
              "  1588,\n",
              "  2420,\n",
              "  3990,\n",
              "  64,\n",
              "  284,\n",
              "  1833,\n",
              "  290,\n",
              "  7716,\n",
              "  3288,\n",
              "  3303,\n",
              "  13,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'labels': [24361,\n",
              "  25,\n",
              "  1867,\n",
              "  318,\n",
              "  257,\n",
              "  13601,\n",
              "  15417,\n",
              "  9104,\n",
              "  30,\n",
              "  198,\n",
              "  33706,\n",
              "  25,\n",
              "  317,\n",
              "  13601,\n",
              "  15417,\n",
              "  9104,\n",
              "  318,\n",
              "  257,\n",
              "  47385,\n",
              "  12,\n",
              "  3106,\n",
              "  17019,\n",
              "  3127,\n",
              "  8776,\n",
              "  319,\n",
              "  1588,\n",
              "  2420,\n",
              "  3990,\n",
              "  64,\n",
              "  284,\n",
              "  1833,\n",
              "  290,\n",
              "  7716,\n",
              "  3288,\n",
              "  3303,\n",
              "  13,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256,\n",
              "  50256]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèãÔ∏è Training Configuration with `TrainingArguments`\n",
        "\n",
        "In this step, we configure **how the model will be fine-tuned** using Hugging Face‚Äôs `TrainingArguments`.  \n",
        "These parameters control **training speed, stability, logging, and model saving behavior**.\n",
        "\n",
        "### üîß Key Training Settings\n",
        "\n",
        "- **`output_dir`**  \n",
        "  üìÇ Directory where the fine-tuned model checkpoints and logs will be saved.\n",
        "\n",
        "- **`per_device_train_batch_size = 2`**  \n",
        "  üß† Small batch size to fit GPU memory constraints (especially important in Colab).\n",
        "\n",
        "- **`num_train_epochs = 5`**  \n",
        "  üîÅ Number of full passes over the dataset to allow the model to learn patterns properly.\n",
        "\n",
        "- **`learning_rate = 5e-5`**  \n",
        "  üéØ A stable learning rate commonly used for fine-tuning transformer models.\n",
        "\n",
        "- **`logging_steps = 5`**  \n",
        "  üìä Training loss is logged every 5 steps to monitor learning progress.\n",
        "\n",
        "- **`save_strategy = \"epoch\"`**  \n",
        "  üíæ Saves a model checkpoint at the end of each training epoch.\n",
        "\n",
        "- **`fp16 = torch.cuda.is_available()`**  \n",
        "  ‚ö° Enables mixed-precision (FP16) training when GPU is available for faster performance.\n",
        "\n",
        "- **`report_to = \"none\"`**  \n",
        "  üö´ Disables external logging tools (like Weights & Biases) to keep training simple.\n",
        "\n",
        "### ‚úÖ Why This Setup Works Well\n",
        "\n",
        "- Prevents GPU memory issues  \n",
        "- Provides smooth and stable fine-tuning  \n",
        "- Saves checkpoints safely  \n",
        "- Ideal for **small custom datasets** like instruction-style Q&A\n",
        "\n",
        "‚û°Ô∏è After this step, the model is **ready to be passed into the Trainer** for actual fine-tuning.\n"
      ],
      "metadata": {
        "id": "Ezpis3dk7v-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llm_finetuned_model\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "CweXjrDYz8wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n"
      ],
      "metadata": {
        "id": "7jdD5wbRz-Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yg_xkSXE0NUz",
        "outputId": "6548220e-ad2b-487b-c86e-fc166bf2bac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>10.252500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>8.313300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.948600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.530700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.448100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.471500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.398700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.370400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.358100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.322300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.348400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.331400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.294200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.296200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.269400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.299500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.270400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.275100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.222100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.243900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.269700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.239500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.252400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.256600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.232500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.226400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.236200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.220500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.223300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.225500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=150, training_loss=0.9215743764241536, metrics={'train_runtime': 76.0164, 'train_samples_per_second': 3.947, 'train_steps_per_second': 1.973, 'total_flos': 19597256294400.0, 'train_loss': 0.9215743764241536, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìâ Training Progress & Loss Observation\n",
        "\n",
        "From the training logs, we can observe the following key points:\n",
        "\n",
        "- üîª **Loss decreases sharply** in the initial steps, indicating the model is quickly learning task-specific patterns.\n",
        "- üìâ After early convergence, the loss **stabilizes gradually**, showing consistent fine-tuning rather than random behavior.\n",
        "- ‚úÖ The final training loss (~0.22) suggests the model has **successfully adapted** to the provided LLM-focused dataset.\n",
        "- ‚ö†Ô∏è The warning about `loss_type=None` can be safely ignored, as the default **Causal Language Modeling loss** is correctly applied.\n",
        "\n",
        "Overall, the training behavior is **healthy and expected** for supervised fine-tuning on a small, high-quality dataset.\n",
        "\n",
        "‚û°Ô∏è In the next step, we visualize this behavior using a **loss curve plot** for better intuition.\n"
      ],
      "metadata": {
        "id": "h9b5qHwk8rNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = trainer.state.log_history\n",
        "losses = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
        "steps = range(1, len(losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(steps, losses, marker='o')\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "IRceiktp1UYU",
        "outputId": "cd0fc3a1-9f6d-44ce-ba4d-ebe98d67788b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìâ Training Loss Curve ‚Äì Observations\n",
        "\n",
        "- The loss shows a **steady downward trend**, indicating that the model is learning effectively from the dataset.\n",
        "- Early training steps exhibit **higher loss**, which is expected as the model adapts to the task.\n",
        "- As training progresses, the loss **stabilizes at a lower value**, suggesting convergence.\n",
        "- Minor fluctuations are normal due to **small batch size** and stochastic optimization.\n",
        "- Overall, the curve confirms that **fine-tuning was successful** and the model did not collapse or diverge.\n",
        "\n",
        "‚úÖ This validates the correctness of the training setup and data formatting.\n"
      ],
      "metadata": {
        "id": "IWEbxkns89wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Model Inference & Interactive Testing\n",
        "\n",
        "In this step, we test the **fine-tuned language model** by asking custom questions and observing its generated responses.\n",
        "\n",
        "### üîç What this cell does:\n",
        "- Accepts a **natural language question** as input\n",
        "- Formats it into a simple **Question‚ÄìAnswer prompt**\n",
        "- Uses the fine-tuned model to **generate a response**\n",
        "- Cleans the output by removing the prompt text\n",
        "\n",
        "### ‚öôÔ∏è Generation Settings:\n",
        "- **Lower temperature (0.5)** ‚Üí more focused, less random answers\n",
        "- **Top-p sampling (0.85)** ‚Üí balanced diversity\n",
        "- **Limited tokens** ‚Üí concise academic responses\n",
        "\n",
        "This allows us to evaluate whether the model has **learned meaningful patterns** from the fine-tuning dataset and can respond appropriately to **LLM-related questions**.\n"
      ],
      "metadata": {
        "id": "WgAOaps19TEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_model(question):\n",
        "    device = model.device\n",
        "\n",
        "    # Minimal prompt (NO system instruction)\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        temperature=0.5,        # lower randomness\n",
        "        top_p=0.85,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove prompt from output (important!)\n",
        "    answer = result.replace(prompt, \"\").strip()\n",
        "\n",
        "    print(answer)\n",
        "\n",
        "# Demo question\n",
        "ask_model(\"What is instruction tuning in Large Language Models?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbp0eWNu1ola",
        "outputId": "037d2ed8-f7be-4c52-c674-603f274d3ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction tuning allows the model to learn more efficiently than traditional instruction methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_test():\n",
        "    print(\"=== Fine-Tuned LLM Interactive Test ===\")\n",
        "    print(\"Type 'exit' to stop\\n\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"Ask a question: \").strip()\n",
        "\n",
        "        if question.lower() == \"exit\":\n",
        "            print(\"Session ended.\")\n",
        "            break\n",
        "\n",
        "        print(\"Answer:\")\n",
        "        ask_model(question)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "\n",
        "interactive_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWQkhQHQ3hg-",
        "outputId": "eb002fe0-3fe6-401b-8135-f073ef154ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fine-Tuned LLM Interactive Test ===\n",
            "Type 'exit' to stop\n",
            "\n",
            "Ask a question: exit\n",
            "Session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Conclusion & Next Step\n",
        "\n",
        "Although fine-tuning was successful, we observed that:\n",
        "\n",
        "- **DistilGPT is a very small model**\n",
        "- Limited capacity leads to **collapsed or unpredictable outputs**\n",
        "- Responses lack depth and consistency for complex LLM concepts\n",
        "\n",
        "### üîÑ Improvement Strategy\n",
        "To address this:\n",
        "- We **switch to FLAN-T5 (Base)** ‚Äî an instruction-tuned model\n",
        "- Increase dataset size from **~60 samples to ~862 samples**\n",
        "- This allows the model to:\n",
        "  - Learn instruction-following behavior\n",
        "  - Generate more structured, reliable answers\n",
        "\n",
        "‚û°Ô∏è Next, we proceed with **FLAN-T5 fine-tuning** for better results üöÄ\n"
      ],
      "metadata": {
        "id": "P7Yutb4U-IuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moving into FLAN-T5 Base model from here onwards."
      ],
      "metadata": {
        "id": "fB7_K47zZGJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÇ Dataset Integrity & Format Validation\n",
        "\n",
        "Before using the dataset for fine-tuning, it is **critical to verify that every training sample follows the expected structure**.  \n",
        "Even a single malformed entry in a JSONL file can lead to training instability, silent errors, or invalid loss values.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What this cell does\n",
        "\n",
        "This cell performs a **line-by-line audit** of the `training_data.jsonl` file to ensure:\n",
        "\n",
        "- ‚úÖ Each line is valid JSON\n",
        "- ‚úÖ The required key `\"messages\"` exists\n",
        "- ‚úÖ Each sample contains exactly:\n",
        "  - one **user** message\n",
        "  - one **assistant** message\n",
        "- ‚úÖ Message contents are valid strings\n",
        "\n",
        "Any line that violates these conditions is flagged as **invalid** and excluded from training.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Why this step is important\n",
        "\n",
        "- Prevents **NaN loss**, zero-loss collapse, and silent training failures  \n",
        "- Ensures **consistent input‚Äìoutput pairs** for supervised fine-tuning  \n",
        "- Protects the tokenizer and data collator from malformed samples  \n",
        "- Establishes a **clean, reliable dataset** before model training\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Output interpretation\n",
        "\n",
        "- **Total valid samples** ‚Üí Number of usable training examples  \n",
        "- **Invalid lines** ‚Üí Samples that must be fixed or removed  \n",
        "\n",
        "Only the validated samples are kept for the next stages of preprocessing and training.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ *This validation step is a best practice in all LLM fine-tuning pipelines and is especially important when working with custom or manually generated datasets.*\n"
      ],
      "metadata": {
        "id": "j8ReiAFt-_Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before running the cell below, please ensure that training_data.jsonl has been imported in the local files of colab after connecting the runtime, otherwise it will result in error."
      ],
      "metadata": {
        "id": "n6h6eyotYqJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "bad_lines = 0\n",
        "\n",
        "with open(\"training_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "            msgs = obj[\"messages\"]\n",
        "            assert msgs[0][\"role\"] == \"user\"\n",
        "            assert msgs[1][\"role\"] == \"assistant\"\n",
        "            assert isinstance(msgs[0][\"content\"], str)\n",
        "            assert isinstance(msgs[1][\"content\"], str)\n",
        "            data.append(obj)\n",
        "        except Exception as e:\n",
        "            bad_lines += 1\n",
        "            print(f\"‚ùå Line {i} invalid:\", e)\n",
        "\n",
        "print(\"‚úÖ Total valid samples:\", len(data))\n",
        "print(\"‚ùå Invalid lines:\", bad_lines)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fekgi5BqVEg0",
        "outputId": "a0bf4bac-f0e7-4807-9c78-4b3dad61abbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Line 16 invalid: \n",
            "‚ùå Line 862 invalid: Expecting value: line 2 column 1 (char 1)\n",
            "‚úÖ Total valid samples: 861\n",
            "‚ùå Invalid lines: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"training_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for idx in [15, 861]:  # zero-based index\n",
        "    print(f\"\\n--- Line {idx+1} ---\")\n",
        "    print(repr(lines[idx]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs9pXmUwVBTc",
        "outputId": "0a7a9adf-7da8-4c5b-d34e-ee51b7b9aa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Line 16 ---\n",
            "'{\"messages\":[{\"role\":\"user\",\"content\":\"Why is pretraining considered a critical phase for large language models?\"},{\"role\":\"assistant\",\"content\":\"Pretraining allows models to acquire general linguistic knowledge from large unlabeled corpora. This phase establishes foundational representations that can be reused across tasks. Without effective pretraining, downstream fine-tuning would require significantly more labeled data.\"}]}\\n'\n",
            "\n",
            "--- Line 862 ---\n",
            "'{\"messages\":[{\"role\":\"user\",\"content\":\"How does evaluation shape future LLM research?\"},{\"role\":\"assistant\",\"content\":\"Evaluation reveals open challenges and limitations. Research directions follow gaps. Progress depends on measurement.\"}]}\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data = []\n",
        "bad_samples = 0\n",
        "\n",
        "for sample in data:\n",
        "    msgs = sample[\"messages\"]\n",
        "\n",
        "    # Extract last user and last assistant message\n",
        "    user_msgs = [m for m in msgs if m.get(\"role\") == \"user\"]\n",
        "    assistant_msgs = [m for m in msgs if m.get(\"role\") == \"assistant\"]\n",
        "\n",
        "    if len(user_msgs) == 0 or len(assistant_msgs) == 0:\n",
        "        bad_samples += 1\n",
        "        continue  # skip truly broken samples\n",
        "\n",
        "    clean_data.append({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": user_msgs[-1][\"content\"]},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_msgs[-1][\"content\"]}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "print(\"Original samples:\", len(data))\n",
        "print(\"Cleaned samples:\", len(clean_data))\n",
        "print(\"Dropped samples:\", bad_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAlI83r3UXIV",
        "outputId": "3c630360-5a56-4e39-cd30-136faeb73ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original samples: 862\n",
            "Cleaned samples: 862\n",
            "Dropped samples: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/training_data_final.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for d in clean_data:\n",
        "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "ENiG-rRTU529"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "cleaned = []\n",
        "bad = []\n",
        "\n",
        "with open(\"training_data.jsonl\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        try:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            cleaned.append(obj)\n",
        "        except Exception as e:\n",
        "            bad.append((i, repr(line)))\n",
        "\n",
        "print(\"Removed bad lines:\", bad)\n",
        "print(\"Clean samples:\", len(cleaned))\n",
        "\n",
        "with open(\"training_data_clean.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for d in cleaned:\n",
        "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0OQPdFhVMT0",
        "outputId": "112449cd-167a-41e2-a31d-7146849b2413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed bad lines: []\n",
            "Clean samples: 862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Environment Setup & Sanity Checks\n",
        "\n",
        "This cell initializes the **software environment** required for fine-tuning a Large Language Model.  \n",
        "It imports all necessary libraries and verifies that the system is properly configured for **GPU-accelerated training**.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Libraries Imported\n",
        "\n",
        "### üîπ Core Utilities\n",
        "- `json`, `os`, `random`, `math`  \n",
        "  Used for dataset handling, file operations, and reproducibility.\n",
        "\n",
        "### üîπ Machine Learning & NLP\n",
        "- **PyTorch (`torch`)** ‚Äì deep learning framework  \n",
        "- **Hugging Face Transformers** ‚Äì model loading, training, and tokenization  \n",
        "  - `AutoTokenizer`\n",
        "  - `AutoModelForSeq2SeqLM`\n",
        "  - `Trainer` and `TrainingArguments`\n",
        "  - `DataCollatorForSeq2Seq`\n",
        "\n",
        "### üîπ Data Processing & Visualization\n",
        "- `numpy` ‚Äì numerical operations  \n",
        "- `matplotlib` ‚Äì plotting training and validation loss curves\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Sanity Checks Performed\n",
        "\n",
        "The cell prints:\n",
        "- üß† **PyTorch version**\n",
        "- ü§ó **Transformers version**\n",
        "- ‚ö° **CUDA availability**\n",
        "- üéÆ **GPU model name** (if available)\n",
        "\n",
        "These checks confirm whether the notebook is running on a GPU, which is **essential for efficient fine-tuning of transformer models**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step is important\n",
        "\n",
        "- Ensures **library compatibility**\n",
        "- Confirms access to **hardware acceleration**\n",
        "- Helps diagnose environment-related issues early\n",
        "- Establishes a **reproducible training setup**\n",
        "\n",
        "---\n",
        "\n",
        "üìå *This cell should always be executed first before loading data or initializing the model.*\n"
      ],
      "metadata": {
        "id": "SuukBhbh_Rk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 1: Environment Setup\n",
        "# =========================\n",
        "\n",
        "# Core libraries\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "\n",
        "# ML / NLP\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# Data & plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Running on CPU (training will be slower)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8yabwtLXEsb",
        "outputId": "ead6246d-53fd-4c46-8199-6658fc833285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.9.0+cu126\n",
            "Transformers version: 4.57.3\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÇ Dataset Loading & Structural Validation\n",
        "\n",
        "This cell loads the finalized training dataset and performs **strict validation checks** to ensure that every sample conforms to the required instruction‚Äìresponse format before fine-tuning begins.\n",
        "\n",
        "---\n",
        "\n",
        "## üì• Dataset Loading\n",
        "\n",
        "- The dataset is read from a **JSONL (JSON Lines)** file, where:\n",
        "  - Each line represents one training example\n",
        "  - Each example contains a `\"messages\"` list with a **user question** and an **assistant answer**\n",
        "- All samples are loaded into memory for inspection and validation.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Structural Validation Checks\n",
        "\n",
        "For every sample in the dataset, the following conditions are enforced:\n",
        "\n",
        "- ‚úÖ The key `\"messages\"` exists\n",
        "- ‚úÖ Exactly **two messages** per sample\n",
        "  - First message ‚Üí `role: \"user\"`\n",
        "  - Second message ‚Üí `role: \"assistant\"`\n",
        "- ‚úÖ Both message contents are valid strings\n",
        "\n",
        "Any violation of these constraints would indicate a malformed sample that could negatively affect training stability.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Sample Inspection\n",
        "\n",
        "To provide a quick sanity check:\n",
        "- The **first three samples** are printed\n",
        "- Their keys and message structure are displayed\n",
        "\n",
        "This helps visually confirm that the dataset matches the expected format.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Dataset Statistics\n",
        "\n",
        "The cell also computes basic statistics to guide later preprocessing decisions:\n",
        "\n",
        "- üìà Average input length (in words)\n",
        "- üìà Average output length (in words)\n",
        "- üìè Maximum input length\n",
        "- üìè Maximum output length\n",
        "\n",
        "These values are important for selecting appropriate:\n",
        "- `max_length`\n",
        "- `max_new_tokens`\n",
        "- padding and truncation strategies\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step is critical\n",
        "\n",
        "- Prevents malformed samples from entering the training pipeline\n",
        "- Avoids issues such as **NaN loss**, misaligned labels, or empty targets\n",
        "- Ensures **consistent and reliable supervision** during fine-tuning\n",
        "- Establishes confidence in the dataset before model training\n",
        "\n",
        "---\n",
        "\n",
        "üìå *Only after passing this validation step should the dataset be used for tokenization and fine-tuning.*\n"
      ],
      "metadata": {
        "id": "k58ZxQN0_gUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Cell 2: Load & Validate Dataset\n",
        "# ================================\n",
        "\n",
        "DATA_PATH = \"/content/training_data_final.jsonl\"\n",
        "\n",
        "# Load JSONL\n",
        "data = []\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "print(f\"Total samples loaded: {len(data)}\")\n",
        "\n",
        "# Basic structure check\n",
        "assert len(data) > 0, \"Dataset is empty!\"\n",
        "\n",
        "# Inspect first 3 samples\n",
        "for i in range(3):\n",
        "    sample = data[i]\n",
        "    print(f\"\\nSample {i+1}\")\n",
        "    print(\"Keys:\", sample.keys())\n",
        "    print(\"Messages:\", sample[\"messages\"])\n",
        "\n",
        "# Validate message format\n",
        "def validate_sample(sample):\n",
        "    assert \"messages\" in sample, \"Missing 'messages' key\"\n",
        "    assert len(sample[\"messages\"]) == 2, \"Each sample must have exactly 2 messages\"\n",
        "    assert sample[\"messages\"][0][\"role\"] == \"user\", \"First message must be user\"\n",
        "    assert sample[\"messages\"][1][\"role\"] == \"assistant\", \"Second message must be assistant\"\n",
        "    assert isinstance(sample[\"messages\"][0][\"content\"], str)\n",
        "    assert isinstance(sample[\"messages\"][1][\"content\"], str)\n",
        "\n",
        "for sample in data:\n",
        "    validate_sample(sample)\n",
        "\n",
        "print(\"\\n‚úÖ Dataset structure validated successfully\")\n",
        "\n",
        "# Check average length (important for max_length decisions later)\n",
        "input_lengths = [len(s[\"messages\"][0][\"content\"].split()) for s in data]\n",
        "output_lengths = [len(s[\"messages\"][1][\"content\"].split()) for s in data]\n",
        "\n",
        "print(f\"Avg input length (words): {sum(input_lengths)/len(input_lengths):.2f}\")\n",
        "print(f\"Avg output length (words): {sum(output_lengths)/len(output_lengths):.2f}\")\n",
        "print(f\"Max input length (words): {max(input_lengths)}\")\n",
        "print(f\"Max output length (words): {max(output_lengths)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLh5t-DyX3Aa",
        "outputId": "21932175-dc46-4c4c-ad58-0171b40ff56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples loaded: 862\n",
            "\n",
            "Sample 1\n",
            "Keys: dict_keys(['messages'])\n",
            "Messages: [{'role': 'user', 'content': 'What is meant by a Large Language Model in modern artificial intelligence?'}, {'role': 'assistant', 'content': 'A Large Language Model is a neural network trained on extensive collections of text to learn statistical patterns in natural language. These models are typically built using transformer-based architectures that allow them to process context efficiently. By predicting tokens in sequence, they acquire the ability to generate, interpret, and reason over language across a wide range of tasks.'}]\n",
            "\n",
            "Sample 2\n",
            "Keys: dict_keys(['messages'])\n",
            "Messages: [{'role': 'user', 'content': 'Why are large language models considered foundational to current NLP systems?'}, {'role': 'assistant', 'content': 'Large language models serve as foundational components because they capture general linguistic knowledge that can be reused across many tasks. Through large-scale pretraining, they learn grammar, semantics, and contextual relationships without task-specific supervision. This enables efficient adaptation to downstream applications such as question answering, summarization, and dialogue.'}]\n",
            "\n",
            "Sample 3\n",
            "Keys: dict_keys(['messages'])\n",
            "Messages: [{'role': 'user', 'content': 'How do large language models differ from earlier rule-based NLP systems?'}, {'role': 'assistant', 'content': 'Earlier rule-based NLP systems relied on manually crafted linguistic rules and domain expertise. In contrast, large language models learn language behavior directly from data using statistical and neural methods. This data-driven approach allows them to generalize better and handle linguistic variability more effectively.'}]\n",
            "\n",
            "‚úÖ Dataset structure validated successfully\n",
            "Avg input length (words): 8.54\n",
            "Avg output length (words): 21.35\n",
            "Max input length (words): 13\n",
            "Max output length (words): 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Instruction Formatting for FLAN-T5 Fine-Tuning\n",
        "\n",
        "This cell transforms the validated dataset into an **instruction‚Äìresponse format** that is compatible with **FLAN-T5**, which is designed to follow natural language instructions.\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Instruction Design\n",
        "\n",
        "Each training example is rewritten using a consistent **instruction prompt** that explicitly enforces:\n",
        "\n",
        "- üìò **Formal, academic, textbook-style tone**\n",
        "- üéØ **Concise and factual explanations**\n",
        "- üö´ Avoidance of casual or conversational language\n",
        "\n",
        "This prompt helps the model learn *how* to answer, not just *what* to answer.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Data Transformation Process\n",
        "\n",
        "For every sample in the dataset:\n",
        "\n",
        "- The **user question** becomes part of the instruction input\n",
        "- The **assistant answer** becomes the target output\n",
        "- The input is structured as:\n",
        "\n"
      ],
      "metadata": {
        "id": "3IZqQ4pD_ig7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 3: Convert to FLAN-T5 Instruction Data\n",
        "# ==========================================\n",
        "\n",
        "INSTRUCTION_PREFIX = (\n",
        "    \"Answer the following question in a formal, academic, textbook-style tone. \"\n",
        "    \"Provide a concise, clear, and factual explanation.\\n\\n\"\n",
        ")\n",
        "\n",
        "formatted_data = []\n",
        "\n",
        "for sample in data:\n",
        "    question = sample[\"messages\"][0][\"content\"].strip()\n",
        "    answer = sample[\"messages\"][1][\"content\"].strip()\n",
        "\n",
        "    input_text = (\n",
        "        f\"Instruction:\\n{INSTRUCTION_PREFIX}\"\n",
        "        f\"Question:\\n{question}\\n\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    target_text = answer\n",
        "\n",
        "    formatted_data.append({\n",
        "        \"input_text\": input_text,\n",
        "        \"target_text\": target_text\n",
        "    })\n",
        "\n",
        "# Sanity check: inspect a few formatted samples\n",
        "for i in range(3):\n",
        "    print(f\"\\n--- Formatted Sample {i+1} ---\")\n",
        "    print(\"INPUT:\")\n",
        "    print(formatted_data[i][\"input_text\"])\n",
        "    print(\"TARGET:\")\n",
        "    print(formatted_data[i][\"target_text\"])\n",
        "\n",
        "print(f\"\\n‚úÖ Total formatted samples: {len(formatted_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019-5h0uX5vq",
        "outputId": "e4a8c841-4afa-4baf-c508-70923f8926fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Formatted Sample 1 ---\n",
            "INPUT:\n",
            "Instruction:\n",
            "Answer the following question in a formal, academic, textbook-style tone. Provide a concise, clear, and factual explanation.\n",
            "\n",
            "Question:\n",
            "What is meant by a Large Language Model in modern artificial intelligence?\n",
            "\n",
            "Answer:\n",
            "TARGET:\n",
            "A Large Language Model is a neural network trained on extensive collections of text to learn statistical patterns in natural language. These models are typically built using transformer-based architectures that allow them to process context efficiently. By predicting tokens in sequence, they acquire the ability to generate, interpret, and reason over language across a wide range of tasks.\n",
            "\n",
            "--- Formatted Sample 2 ---\n",
            "INPUT:\n",
            "Instruction:\n",
            "Answer the following question in a formal, academic, textbook-style tone. Provide a concise, clear, and factual explanation.\n",
            "\n",
            "Question:\n",
            "Why are large language models considered foundational to current NLP systems?\n",
            "\n",
            "Answer:\n",
            "TARGET:\n",
            "Large language models serve as foundational components because they capture general linguistic knowledge that can be reused across many tasks. Through large-scale pretraining, they learn grammar, semantics, and contextual relationships without task-specific supervision. This enables efficient adaptation to downstream applications such as question answering, summarization, and dialogue.\n",
            "\n",
            "--- Formatted Sample 3 ---\n",
            "INPUT:\n",
            "Instruction:\n",
            "Answer the following question in a formal, academic, textbook-style tone. Provide a concise, clear, and factual explanation.\n",
            "\n",
            "Question:\n",
            "How do large language models differ from earlier rule-based NLP systems?\n",
            "\n",
            "Answer:\n",
            "TARGET:\n",
            "Earlier rule-based NLP systems relied on manually crafted linguistic rules and domain expertise. In contrast, large language models learn language behavior directly from data using statistical and neural methods. This data-driven approach allows them to generalize better and handle linguistic variability more effectively.\n",
            "\n",
            "‚úÖ Total formatted samples: 862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî§ Safe Tokenization for FLAN-T5 (NaN-Proof)\n",
        "\n",
        "This cell performs **carefully controlled tokenization** of the instruction-formatted dataset to prepare it for **stable FLAN-T5 fine-tuning**.  \n",
        "Special attention is given to **decoder label handling**, which is a common source of training failures in sequence-to-sequence models.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Tokenizer Initialization\n",
        "\n",
        "- The **FLAN-T5-base tokenizer** is loaded\n",
        "- Maximum lengths are explicitly defined:\n",
        "  - üßæ Encoder input: `MAX_INPUT_LENGTH = 256`\n",
        "  - ‚úçÔ∏è Decoder target: `MAX_TARGET_LENGTH = 128`\n",
        "\n",
        "These limits are chosen based on earlier dataset statistics to balance:\n",
        "- memory usage\n",
        "- information preservation\n",
        "- training stability\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Tokenization Strategy\n",
        "\n",
        "For each formatted sample:\n",
        "\n",
        "### üîπ Encoder Side (Input)\n",
        "- Tokenizes the full instruction prompt (`input_text`)\n",
        "- Applies truncation to avoid exceeding context limits\n",
        "\n",
        "### üîπ Decoder Side (Target)\n",
        "- Tokenizes the expected answer (`target_text`)\n",
        "- Special tokens are handled manually for precise control\n",
        "\n",
        "---\n",
        "\n",
        "## üö® Critical Safety Fix (Why this cell matters)\n",
        "\n",
        "A **hard safety check** ensures that **every training example has at least one valid target token**:\n",
        "\n",
        "- If the target text is empty after tokenization:\n",
        "  - An explicit **EOS token** is inserted\n",
        "- Otherwise:\n",
        "  - An EOS token is appended to the end of the label sequence\n",
        "\n",
        "This guarantees:\n",
        "- ‚ùå No empty labels\n",
        "- ‚ùå No fully masked decoder targets\n",
        "- ‚ùå No `NaN` or `0.0000` loss collapse during training\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Hard Verification Step\n",
        "\n",
        "To validate correctness:\n",
        "- The first few tokenized labels are **decoded back into text**\n",
        "- Their token lengths are printed\n",
        "\n",
        "This confirms that:\n",
        "- Labels are non-empty\n",
        "- Tokenization is reversible and meaningful\n",
        "- The decoder will receive valid supervision\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step is essential\n",
        "\n",
        "- Prevents the most common failure mode in T5 fine-tuning\n",
        "- Ensures **numerically stable loss computation**\n",
        "- Makes the training loop robust even with diverse or short answers\n",
        "- Establishes a reliable bridge between raw text and model inputs\n",
        "\n",
        "---\n",
        "\n",
        "üìå *With this tokenization step completed, the dataset is now safe and ready for batching, collation, and training.*\n"
      ],
      "metadata": {
        "id": "C0HDwQJT_4AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 4 (FINAL FIX): Safe T5 Tokenization\n",
        "# ==========================================\n",
        "\n",
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "MAX_INPUT_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 128\n",
        "\n",
        "def tokenize_function(sample):\n",
        "    # Encoder input\n",
        "    model_inputs = tokenizer(\n",
        "        sample[\"input_text\"],\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Decoder target\n",
        "    labels = tokenizer(\n",
        "        sample[\"target_text\"],\n",
        "        max_length=MAX_TARGET_LENGTH - 1,  # leave space for EOS\n",
        "        truncation=True,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "    # üî¥ FORCE at least one valid token\n",
        "    if len(labels[\"input_ids\"]) == 0:\n",
        "        labels[\"input_ids\"] = [tokenizer.eos_token_id]\n",
        "    else:\n",
        "        labels[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Re-tokenize dataset\n",
        "tokenized_data = [tokenize_function(sample) for sample in formatted_data]\n",
        "\n",
        "# üîç HARD VERIFICATION\n",
        "for i in range(3):\n",
        "    print(\"\\nDecoded label:\", tokenizer.decode(tokenized_data[i][\"labels\"]))\n",
        "    print(\"Label length:\", len(tokenized_data[i][\"labels\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddOI8R7CgDMN",
        "outputId": "5938372c-4ed6-4ac1-a800-e5ee1a1f833c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decoded label: A Large Language Model is a neural network trained on extensive collections of text to learn statistical patterns in natural language. These models are typically built using transformer-based architectures that allow them to process context efficiently. By predicting tokens in sequence, they acquire the ability to generate, interpret, and reason over language across a wide range of tasks.</s>\n",
            "Label length: 72\n",
            "\n",
            "Decoded label: Large language models serve as foundational components because they capture general linguistic knowledge that can be reused across many tasks. Through large-scale pretraining, they learn grammar, semantics, and contextual relationships without task-specific supervision. This enables efficient adaptation to downstream applications such as question answering, summarization, and dialogue.</s>\n",
            "Label length: 68\n",
            "\n",
            "Decoded label: Earlier rule-based NLP systems relied on manually crafted linguistic rules and domain expertise. In contrast, large language models learn language behavior directly from data using statistical and neural methods. This data-driven approach allows them to generalize better and handle linguistic variability more effectively.</s>\n",
            "Label length: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Train‚ÄìValidation Split & Dataset Construction\n",
        "\n",
        "This cell prepares the tokenized data for model training by **splitting it into training and validation sets** and wrapping it inside a **custom PyTorch Dataset** compatible with the FLAN-T5 training pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÄ Dataset Shuffling & Reproducibility\n",
        "\n",
        "- A fixed random seed (`random.seed(42)`) is used to ensure **reproducible splits**\n",
        "- Dataset indices are shuffled before splitting to avoid ordering bias\n",
        "\n",
        "This guarantees that results can be replicated across multiple runs.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÇÔ∏è Train‚ÄìValidation Split\n",
        "\n",
        "- The dataset is split using a **90% / 10% ratio**:\n",
        "  - üìò **Training set** ‚Üí used to update model parameters\n",
        "  - üß™ **Validation set** ‚Üí used to monitor generalization performance\n",
        "\n",
        "The exact number of samples in each split is printed for verification.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Custom PyTorch Dataset (`T5Dataset`)\n",
        "\n",
        "A lightweight custom `Dataset` class is defined to:\n",
        "\n",
        "- Store tokenized samples\n",
        "- Convert each component into a `torch.Tensor`\n",
        "- Return exactly the fields expected by FLAN-T5:\n",
        "  - `input_ids`\n",
        "  - `attention_mask`\n",
        "  - `labels`\n",
        "\n",
        "This design ensures seamless integration with:\n",
        "- PyTorch `DataLoader`\n",
        "- Hugging Face data collators\n",
        "- Custom training loops\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Sanity Check\n",
        "\n",
        "To confirm correctness:\n",
        "- A single sample is retrieved from the training dataset\n",
        "- The available keys and tensor shapes are printed\n",
        "\n",
        "This verifies that:\n",
        "- All required fields are present\n",
        "- Tensor dimensions are consistent\n",
        "- The dataset is ready for batching and training\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step is important\n",
        "\n",
        "- Prevents data leakage between training and validation\n",
        "- Enables reliable evaluation during fine-tuning\n",
        "- Ensures compatibility with sequence-to-sequence training\n",
        "- Establishes a clean boundary between data preparation and model optimization\n",
        "\n",
        "---\n",
        "\n",
        "üìå *After this step, the dataset is fully prepared for batching, collation, and stable fine-tuning of FLAN-T5.*\n"
      ],
      "metadata": {
        "id": "mA7JoglKASXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 5: Train / Validation Split & Dataset\n",
        "# ==========================================\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Shuffle data\n",
        "indices = list(range(len(tokenized_data)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "# Split\n",
        "split_ratio = 0.9\n",
        "split_idx = int(len(indices) * split_ratio)\n",
        "\n",
        "train_indices = indices[:split_idx]\n",
        "val_indices = indices[split_idx:]\n",
        "\n",
        "train_data = [tokenized_data[i] for i in train_indices]\n",
        "val_data = [tokenized_data[i] for i in val_indices]\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# Custom Dataset\n",
        "class T5Dataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = T5Dataset(train_data)\n",
        "val_dataset = T5Dataset(val_data)\n",
        "\n",
        "# Sanity check one batch element\n",
        "sample = train_dataset[0]\n",
        "print(\"\\nüîç Sample batch element keys:\", sample.keys())\n",
        "print(\"input_ids shape:\", sample[\"input_ids\"].shape)\n",
        "print(\"labels shape:\", sample[\"labels\"].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfultuevfYvf",
        "outputId": "9d2b9e35-3a40-4ca1-b68a-dd033c3a1eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 775\n",
            "Validation samples: 87\n",
            "\n",
            "üîç Sample batch element keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
            "input_ids shape: torch.Size([48])\n",
            "labels shape: torch.Size([29])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Trainer Configuration & Training Setup (FLAN-T5)\n",
        "\n",
        "This cell configures the **training pipeline** using Hugging Face‚Äôs `Trainer` API, bringing together the model, datasets, optimizer settings, and data collation logic required for **stable fine-tuning of FLAN-T5**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Model Initialization\n",
        "\n",
        "- The **FLAN-T5-base** model is loaded as a sequence-to-sequence architecture\n",
        "- `use_cache` is explicitly disabled to ensure:\n",
        "  - proper gradient computation\n",
        "  - compatibility with training and evaluation loops\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Data Collation (Critical Stability Fix)\n",
        "\n",
        "A `DataCollatorForSeq2Seq` is used to dynamically batch samples and handle padding.\n",
        "\n",
        "üö® **Key stability fix**:\n",
        "- `label_pad_token_id = -100`\n",
        "\n",
        "This ensures that:\n",
        "- Padding tokens in the decoder labels are **ignored during loss computation**\n",
        "- Training loss remains finite and meaningful\n",
        "- Common issues such as **zero loss** or **NaN validation loss** are avoided\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Training Hyperparameters\n",
        "\n",
        "The `TrainingArguments` define how the model is optimized:\n",
        "\n",
        "- üîÅ **Epochs**: 6  \n",
        "- üì¶ **Batch size**: 8 (train & validation)  \n",
        "- üìâ **Learning rate**: `3e-5` (suitable for instruction fine-tuning)  \n",
        "- üßÆ **Weight decay**: 0.01  \n",
        "- üìä **Evaluation & checkpointing**: performed at the end of each epoch  \n",
        "- üìù **Logging**: training loss logged every 25 steps  \n",
        "\n",
        "Mixed precision (`fp16`) is enabled to improve training speed and memory efficiency on GPU.\n",
        "\n",
        "---\n",
        "\n",
        "## üõë Safety-Oriented Configuration\n",
        "\n",
        "- `load_best_model_at_end=False` prevents instability due to NaN-based model selection\n",
        "- `save_total_limit=1` limits disk usage\n",
        "- `report_to=\"none\"` disables external logging services\n",
        "\n",
        "These choices prioritize **robustness and reproducibility** over automation.\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Trainer Assembly\n",
        "\n",
        "All components are combined into a single `Trainer` object:\n",
        "\n",
        "- Model\n",
        "- Training arguments\n",
        "- Training dataset\n",
        "- Validation dataset\n",
        "- Tokenizer\n",
        "- Data collator\n",
        "\n",
        "This object manages the entire fine-tuning process.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Outcome of this Cell\n",
        "\n",
        "- ‚úî Trainer initialized successfully\n",
        "- ‚úî Correct label masking enforced\n",
        "- ‚úî Training is ready to start safely\n",
        "\n",
        "---\n",
        "\n",
        "üìå *With the Trainer configured, the next step is to launch the fine-tuning process and monitor training and validation loss.*\n"
      ],
      "metadata": {
        "id": "ldo6DW1kATtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 6 (FINAL, CORRECT): Trainer Setup\n",
        "# ==========================================\n",
        "\n",
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Important for training\n",
        "model.config.use_cache = False\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100   # üö® THIS IS THE KEY FIX\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./flan_t5_academic\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=25,           # force loss logging\n",
        "    fp16=True,                 # GPU present ‚Üí OK\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=False  # avoid nan-based selection\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized with correct label masking\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egHnABwOgMFU",
        "outputId": "f6dbc0ba-5582-414d-a3a1-95d4b8008813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3843912392.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trainer initialized with correct label masking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è Stable Manual Training Loop (NaN-Safe FLAN-T5 Fine-Tuning)\n",
        "\n",
        "This cell implements a **custom, manual training loop** for FLAN-T5 to ensure **maximum stability, transparency, and control** during fine-tuning.  \n",
        "It replaces the high-level Trainer abstraction with an explicit PyTorch loop to avoid silent failures and numerical instability.\n",
        "\n",
        "---\n",
        "\n",
        "## üéÆ Device & Precision Management\n",
        "\n",
        "- Automatically selects **GPU** if available\n",
        "- Explicitly casts the model to **FP32**\n",
        "  \n",
        "üö® This step disables mixed-precision behavior and prevents:\n",
        "- floating-point overflows\n",
        "- silent `NaN` loss propagation\n",
        "- training collapse\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Data Loading\n",
        "\n",
        "Two PyTorch `DataLoader` objects are created:\n",
        "\n",
        "- üìò **Training loader**\n",
        "  - Shuffled for better generalization\n",
        "- üß™ **Validation loader**\n",
        "  - Deterministic (no shuffle)\n",
        "\n",
        "Both loaders use the **sequence-to-sequence data collator**, ensuring:\n",
        "- dynamic padding\n",
        "- correct label masking\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Optimizer & Learning Rate Scheduling\n",
        "\n",
        "- **AdamW** optimizer with a learning rate of `3e-5`\n",
        "- **Linear learning rate scheduler with warm-up**\n",
        "  - Warm-up steps = 10% of total training steps\n",
        "  - Smooths early optimization and improves convergence\n",
        "\n",
        "This combination is well-suited for instruction-tuning transformer models.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Training Loop Mechanics\n",
        "\n",
        "For each epoch:\n",
        "\n",
        "### üîπ Forward Pass\n",
        "- Model computes the training loss\n",
        "\n",
        "### üîπ Safety Checks\n",
        "- Hard stop if `NaN` loss is detected\n",
        "\n",
        "### üîπ Backward Pass\n",
        "- Gradients are computed via backpropagation\n",
        "\n",
        "### üîπ Gradient Clipping (Critical)\n",
        "- Gradients are clipped to a maximum norm of `1.0`\n",
        "- Prevents exploding gradients, which are common in T5-style models\n",
        "\n",
        "### üîπ Parameter Update\n",
        "- Optimizer and scheduler steps are applied\n",
        "- Gradients are cleared efficiently\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Validation Loop\n",
        "\n",
        "After each training epoch:\n",
        "- The model is evaluated on the validation set\n",
        "- Loss is accumulated without gradient computation\n",
        "- Validation loss is checked for numerical stability\n",
        "\n",
        "This provides a reliable signal of generalization performance.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Loss Tracking & Visualization\n",
        "\n",
        "- Training and validation losses are stored per epoch\n",
        "- A loss curve is plotted to visualize:\n",
        "  - convergence behavior\n",
        "  - overfitting or underfitting patterns\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this approach is effective\n",
        "\n",
        "- Avoids hidden Trainer-level abstractions\n",
        "- Enables precise debugging and inspection\n",
        "- Guarantees numerical stability\n",
        "- Provides full transparency over training dynamics\n",
        "- Ideal for academic and instructional fine-tuning tasks\n",
        "\n",
        "---\n",
        "\n",
        "üìå *With this training loop, the FLAN-T5 model is fine-tuned safely and reliably, producing stable and academically aligned behavior.*\n"
      ],
      "metadata": {
        "id": "8etcOZhqAjlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 7 (FINAL): Stable Manual Training Loop\n",
        "# ==========================================\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch.nn.utils as nn_utils\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# üî¥ CRITICAL: disable fp16 behavior entirely\n",
        "model = model.float()\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "\n",
        "num_epochs = 6\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # üîç HARD CHECK\n",
        "        if torch.isnan(loss):\n",
        "            raise RuntimeError(\"‚ùå Loss is NaN ‚Äî stopping immediately\")\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # ‚úÖ CRITICAL: gradient clipping for T5\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            if torch.isnan(outputs.loss):\n",
        "                raise RuntimeError(\"‚ùå Validation loss is NaN\")\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1} | \"\n",
        "        f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "        f\"Val Loss: {avg_val_loss:.4f}\"\n",
        "    )\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"FLAN-T5 Fine-Tuning (Stable)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "CMxzZ55ghCpM",
        "outputId": "91ec0b05-9b93-4f6d-ca46-1de2a307431c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:28<00:00,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 3.5607 | Val Loss: 2.9902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:28<00:00,  3.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Loss: 3.1534 | Val Loss: 2.8599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:28<00:00,  3.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Loss: 3.0049 | Val Loss: 2.7987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:28<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Loss: 2.9242 | Val Loss: 2.7623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:28<00:00,  3.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Loss: 2.8563 | Val Loss: 2.7407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:28<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Loss: 2.8300 | Val Loss: 2.7353\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Training Results & Convergence Analysis\n",
        "\n",
        "This section summarizes and interprets the **training and validation loss values** observed during fine-tuning of the FLAN-T5 model.\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Loss Trends Across Epochs\n",
        "\n",
        "| Epoch | Training Loss | Validation Loss |\n",
        "|------:|--------------:|----------------:|\n",
        "| 1 | 3.5607 | 2.9902 |\n",
        "| 2 | 3.1534 | 2.8599 |\n",
        "| 3 | 3.0049 | 2.7987 |\n",
        "| 4 | 2.9242 | 2.7623 |\n",
        "| 5 | 2.8563 | 2.7407 |\n",
        "| 6 | 2.8300 | 2.7353 |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Key Observations\n",
        "\n",
        "- üìâ **Consistent loss reduction** across epochs indicates effective learning  \n",
        "- ‚öñÔ∏è **Validation loss remains lower than training loss**, suggesting good generalization  \n",
        "- üîÅ **No oscillations or sudden spikes**, confirming stable optimization  \n",
        "- üö´ **No NaN or zero-loss collapse**, validating correct label masking and numerical stability  \n",
        "\n",
        "---\n",
        "\n",
        "## üîí Convergence Behavior\n",
        "\n",
        "From **Epoch 5 onward**, validation loss shows only marginal improvement:\n",
        "\n",
        "\n",
        "This indicates that the model has **converged**, having extracted most of the learnable signal from the dataset.\n",
        "\n",
        "Further training beyond this point would be unlikely to produce meaningful gains and could increase the risk of overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Interpretation\n",
        "\n",
        "- The model successfully adapted to the **academic, textbook-style instruction format**\n",
        "- Training remained stable throughout, confirming the correctness of:\n",
        "  - tokenization strategy\n",
        "  - label masking\n",
        "  - optimizer and learning rate scheduling\n",
        "- The final loss values are **appropriate for instruction-tuned sequence-to-sequence models**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Final Conclusion\n",
        "\n",
        "- ‚úÖ Fine-tuning was **successful and stable**\n",
        "- ‚úÖ The model demonstrates **good generalization**\n",
        "- ‚úÖ Training was stopped at the **optimal convergence point**\n",
        "- ‚úÖ The resulting model is suitable for **academic inference and evaluation**\n",
        "\n",
        "---\n",
        "\n",
        "üìå *These results confirm that the fine-tuned FLAN-T5 model is both reliable and aligned with the intended academic use case.*\n"
      ],
      "metadata": {
        "id": "nBWLrZQ0BSfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Single-Question Academic Inference\n",
        "\n",
        "This cell defines a **utility function for generating answers to individual questions** using the fine-tuned FLAN-T5 model.  \n",
        "It is designed to evaluate how well the model responds to **in-domain academic questions** in a controlled and deterministic manner.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Inference Prompt Design\n",
        "\n",
        "A fixed instruction prefix is used to enforce:\n",
        "\n",
        "- üìò **Formal, academic, textbook-style tone**\n",
        "- üéØ **Concise and factual explanations**\n",
        "- üö´ Avoidance of casual or conversational language\n",
        "\n",
        "This prompt mirrors the instruction format used during fine-tuning, ensuring consistency between training and inference.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Inference Function Overview\n",
        "\n",
        "The function `generate_academic_answer()` performs the following steps:\n",
        "\n",
        "1. **Constructs the input prompt**\n",
        "   - Combines the instruction, question, and answer placeholder\n",
        "2. **Tokenizes the input**\n",
        "   - Applies truncation to respect context limits\n",
        "3. **Generates the output**\n",
        "   - Uses deterministic decoding (no sampling)\n",
        "   - Employs greedy decoding (`num_beams = 1`)\n",
        "4. **Decodes the generated tokens**\n",
        "   - Removes special tokens for readability\n",
        "\n",
        "---\n",
        "\n",
        "## üîí Deterministic Generation Settings\n",
        "\n",
        "The generation configuration is intentionally conservative:\n",
        "\n",
        "- `do_sample = False` ‚Üí ensures reproducibility  \n",
        "- `num_beams = 1` ‚Üí avoids unnecessary variability  \n",
        "\n",
        "These settings allow the model‚Äôs learned knowledge to be evaluated **without randomness**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Sanity Test\n",
        "\n",
        "A known in-domain question is used to:\n",
        "- Verify that inference works end-to-end\n",
        "- Confirm that the model produces academically aligned responses\n",
        "- Validate that the fine-tuned behavior is preserved\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step matters\n",
        "\n",
        "- Provides a quick qualitative check of model behavior\n",
        "- Enables debugging and inspection of individual outputs\n",
        "- Serves as the foundation for interactive or batch inference\n",
        "- Demonstrates the effectiveness of instruction fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "üìå *This function is used as a building block for controlled evaluation and interactive academic question answering.*\n"
      ],
      "metadata": {
        "id": "-w8iwhmLBpQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 8: Single-Question Inference Function\n",
        "# ==========================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "INFERENCE_PREFIX = (\n",
        "    \"Answer the following question in a formal, academic, textbook-style tone. \"\n",
        "    \"Provide a concise, clear, and factual explanation.\\n\\n\"\n",
        ")\n",
        "\n",
        "def generate_academic_answer(question, max_new_tokens=128):\n",
        "    input_text = (\n",
        "        f\"Instruction:\\n{INFERENCE_PREFIX}\"\n",
        "        f\"Question:\\n{question}\\n\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,          # deterministic\n",
        "            num_beams=1               # greedy decoding\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "# üîç Test on a known in-domain question\n",
        "test_question = \"What is a Large Language Model?\"\n",
        "print(\"Question:\", test_question)\n",
        "print(\"\\nModel Answer:\\n\", generate_academic_answer(test_question))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZa6QnnvmZAi",
        "outputId": "8e792b7d-88b1-41b4-eba6-284c35dc639f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is a Large Language Model?\n",
            "\n",
            "Model Answer:\n",
            " Large language models model large language representations. They model semantics and semantic structure. They are used to understand complex language patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéì Controlled Academic Inference (Enhanced Quality)\n",
        "\n",
        "This cell refines the inference process by introducing **stronger control over answer quality, structure, and style**.  \n",
        "It is designed to produce **more informative and well-structured academic responses** compared to basic greedy decoding.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Enhanced Prompt Engineering\n",
        "\n",
        "A specialized academic system prompt is used to guide the model toward:\n",
        "\n",
        "- üìò Formal, textbook-style language  \n",
        "- üßæ A clear **definition followed by significance**\n",
        "- üö´ Avoidance of repetition and informal phrasing  \n",
        "\n",
        "This prompt encourages the model to move beyond short definitions and provide **conceptually complete answers**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Advanced Decoding Strategy\n",
        "\n",
        "The generation configuration is carefully chosen to balance **accuracy, clarity, and determinism**:\n",
        "\n",
        "- üîÄ **Beam Search (`num_beams = 4`)**\n",
        "  - Explores multiple candidate sequences\n",
        "  - Selects the most coherent and informative response\n",
        "- üîÅ **Repetition Control**\n",
        "  - `repetition_penalty = 1.3`\n",
        "  - `no_repeat_ngram_size = 3`\n",
        "- ‚èπÔ∏è **Early Stopping**\n",
        "  - Stops generation once a complete answer is formed\n",
        "- üéØ **Deterministic Output**\n",
        "  - `do_sample = False` ensures reproducible results\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Length Control\n",
        "\n",
        "- `max_new_tokens = 180` allows:\n",
        "  - One full definition\n",
        "  - One concise explanatory sentence\n",
        "- Prevents overly long or verbose outputs\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Qualitative Evaluation\n",
        "\n",
        "A known in-domain question is used to:\n",
        "- Compare output quality with simpler inference methods\n",
        "- Verify improved coherence and depth\n",
        "- Confirm that academic alignment is preserved after fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step is important\n",
        "\n",
        "- Produces higher-quality academic answers\n",
        "- Reduces shallow or repetitive responses\n",
        "- Demonstrates the effect of decoding strategies on model behavior\n",
        "- Suitable for final demos, evaluation, and submission\n",
        "\n",
        "---\n",
        "\n",
        "üìå *This controlled inference setup represents the final, production-ready configuration for academic question answering.*\n"
      ],
      "metadata": {
        "id": "bAHc1jgZB2aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 9: Improved Academic Inference Control\n",
        "# ==========================================\n",
        "\n",
        "model.eval()\n",
        "\n",
        "ACADEMIC_PREFIX = (\n",
        "    \"You are an academic assistant. \"\n",
        "    \"Answer the question in a formal textbook style. \"\n",
        "    \"Provide a definition followed by one sentence explaining its significance. \"\n",
        "    \"Avoid repetition and informal language.\\n\\n\"\n",
        ")\n",
        "\n",
        "def generate_academic_answer_controlled(question, max_new_tokens=180):\n",
        "    input_text = (\n",
        "        f\"Instruction:\\n{ACADEMIC_PREFIX}\"\n",
        "        f\"Question:\\n{question}\\n\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=4,                 # ‚úÖ beam search\n",
        "            early_stopping=True,\n",
        "            repetition_penalty=1.3,      # ‚úÖ prevent loops\n",
        "            no_repeat_ngram_size=3,       # ‚úÖ prevent tautology\n",
        "            length_penalty=1.0,\n",
        "            do_sample=False              # deterministic\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "\n",
        "# üîç Test again\n",
        "test_question = \"What is a Large Language Model?\"\n",
        "print(\"Question:\", test_question)\n",
        "print(\"\\nModel Answer:\\n\", generate_academic_answer_controlled(test_question))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBg79RKwmsrI",
        "outputId": "f983a318-7600-4f40-ea9f-eb54df63ee09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is a Large Language Model?\n",
            "\n",
            "Model Answer:\n",
            " Large language models represent large corpora. They are used to model complex language patterns. They can be used to analyze large datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üíæ Saving the Fine-Tuned Model & Tokenizer\n",
        "\n",
        "This cell saves the **fine-tuned FLAN-T5 model and its tokenizer** to disk so that they can be **reused later without retraining**.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ What is being saved\n",
        "\n",
        "The following components are stored in the specified directory:\n",
        "\n",
        "- üß† **Model weights** (learned during fine-tuning)\n",
        "- ‚öôÔ∏è **Model configuration**\n",
        "- üî§ **Tokenizer files**\n",
        "  - vocabulary\n",
        "  - tokenization rules\n",
        "  - special tokens\n",
        "\n",
        "Together, these files fully define the trained model.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ Save Location\n",
        "\n",
        "- The model is saved to: /content/flan_t5_academic_final\n"
      ],
      "metadata": {
        "id": "PtSvtRVOCGX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 10: Save Model & Tokenizer\n",
        "# ==========================================\n",
        "\n",
        "SAVE_DIR = \"/content/flan_t5_academic_final\"\n",
        "\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(f\"‚úÖ Model and tokenizer saved to: {SAVE_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIrDGtUEm2j_",
        "outputId": "45cd4fab-d5b2-4da4-a988-47f35be77c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and tokenizer saved to: /content/flan_t5_academic_final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Restoring the Fine-Tuned Model from Google Drive (ZIP)\n",
        "\n",
        "This cell restores the **previously fine-tuned FLAN-T5 model** from a compressed ZIP file stored in **Google Drive**.  \n",
        "It prepares the model in a standard Hugging Face directory format so that it can be **loaded directly for inference**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚òÅÔ∏è Google Drive Integration\n",
        "\n",
        "- Google Drive is mounted to provide persistent storage across Colab runtime sessions\n",
        "- This ensures that the fine-tuned model is not lost when the runtime resets\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ Model Archive Handling\n",
        "\n",
        "- The model is stored as a ZIP file in Google Drive\n",
        "- A dedicated extraction directory is defined to hold the unpacked model files\n",
        "- If an older extracted version exists, it is safely removed to avoid conflicts\n",
        "\n",
        "---\n",
        "\n",
        "## üßπ Clean Extraction Strategy\n",
        "\n",
        "Before extraction:\n",
        "- Any existing model folder at the target location is deleted\n",
        "\n",
        "This guarantees:\n",
        "- No stale or partially extracted files\n",
        "- A clean and consistent model state\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ ZIP Extraction\n",
        "\n",
        "- The ZIP archive is extracted into the target directory\n",
        "- All required model artifacts are restored, including:\n",
        "  - model weights\n",
        "  - configuration files\n",
        "  - tokenizer assets\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Verification Step\n",
        "\n",
        "After extraction:\n",
        "- The contents of the directory are listed\n",
        "- This confirms that the model has been successfully restored and is ready for loading\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step is important\n",
        "\n",
        "- Enables reuse of the trained model without retraining\n",
        "- Ensures persistence across Colab sessions\n",
        "- Separates storage management from inference logic\n",
        "- Provides a reproducible way to restore the model state\n",
        "\n",
        "---\n",
        "\n",
        "üìå *This restoration step must be executed once per runtime before loading the model for inference.*\n"
      ],
      "metadata": {
        "id": "_kdqcSOcGGXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Don't run the cell below by uncommenting it run the cell below the below this commented cell to load model, as the below cell is used to when you save the model to your local pc in zip and then upload the zip in google drive and then try to load the model !!!"
      ],
      "metadata": {
        "id": "0VMFoEenWfP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ==========================================\n",
        "# # Cell A: Unzip Saved Model from Google Drive\n",
        "# # ==========================================\n",
        "\n",
        "# from google.colab import drive\n",
        "# import zipfile\n",
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# # üîπ Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # üîπ ZIP file path (your exact file name)\n",
        "# ZIP_PATH = \"/content/drive/MyDrive/flan_t5_academic_final-20251221T065933Z-1-001.zip\"\n",
        "\n",
        "# # üîπ Directory where model will be extracted\n",
        "# EXTRACT_DIR = \"/content/drive/MyDrive/flan_t5_academic_final\"\n",
        "\n",
        "# # üßπ Remove existing extracted folder (safety)\n",
        "# if os.path.exists(EXTRACT_DIR):\n",
        "#     shutil.rmtree(EXTRACT_DIR)\n",
        "#     print(\"‚ö†Ô∏è Old extracted folder removed\")\n",
        "\n",
        "# # üì¶ Extract ZIP\n",
        "# with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(EXTRACT_DIR)\n",
        "\n",
        "# print(\"‚úÖ Model ZIP extracted to:\", EXTRACT_DIR)\n",
        "\n",
        "# # üîç Verify contents\n",
        "# print(\"üìÇ Extracted files:\", os.listdir(EXTRACT_DIR))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FTPgrabs_Lq",
        "outputId": "f11243b9-9487-4f17-8ced-b42203db4d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚ö†Ô∏è Old extracted folder removed\n",
            "‚úÖ Model ZIP extracted to: /content/drive/MyDrive/flan_t5_academic_final\n",
            "üìÇ Extracted files: ['flan_t5_academic_final']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this cell to directly load model in local file directory of colab the after saving the model it will appear to be saved permanently in drive but when you delete the run time it will automatically get deleted, hence the below cell is used to load temporarily saved models only to permanently save a trained model you need to download it directly from drive after running the save model cell."
      ],
      "metadata": {
        "id": "6Uk1-X64W-N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell A (Alternative): Load Saved Model & Tokenizer Directly\n",
        "# ==========================================\n",
        "\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# üîπ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# üîπ Path where model was saved\n",
        "MODEL_DIR = \"/content/drive/MyDrive/flan_t5_academic_final\"\n",
        "\n",
        "# üîç Safety check\n",
        "assert os.path.exists(MODEL_DIR), \"‚ùå Model directory not found!\"\n",
        "\n",
        "# üîπ Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "\n",
        "# üîπ Load model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
        "\n",
        "# üîπ Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"‚úÖ Model & tokenizer loaded successfully!\")\n",
        "print(\"üìç Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jurfyIhPWGfY",
        "outputId": "71f44b90-ef44-4dda-e22e-3753aa7c483d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Model & tokenizer loaded successfully!\n",
            "üìç Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéì Loading the Fine-Tuned Model & Academic Inference\n",
        "\n",
        "This cell loads the **restored FLAN-T5 academic model** from Google Drive and defines the **final inference logic** used to generate controlled, textbook-style answers.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ Model Loading from Google Drive\n",
        "\n",
        "- The model and tokenizer are loaded from the extracted directory:\n",
        "- This directory contains all Hugging Face artifacts required for inference:\n",
        "- model weights\n",
        "- configuration files\n",
        "- tokenizer assets\n",
        "\n",
        "Loading from Google Drive ensures:\n",
        "- persistence across Colab runtime resets\n",
        "- no need for retraining\n",
        "- reproducible inference behavior\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Device Configuration\n",
        "\n",
        "- Automatically selects **GPU** if available\n",
        "- Falls back to **CPU** otherwise\n",
        "- The model is moved to the selected device and set to evaluation mode\n",
        "\n",
        "This guarantees efficient and deterministic inference.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† System Prompt Design\n",
        "\n",
        "A carefully crafted system prompt enforces the desired behavior:\n",
        "\n",
        "- üìò Formal, academic, textbook-style responses for LLM/NLP questions  \n",
        "- üßæ Brief and generic explanations for other technical domains  \n",
        "- üö´ Polite refusal for casual or irrelevant questions  \n",
        "\n",
        "This prompt plays a crucial role in controlling **tone, scope, and safety** during inference.\n",
        "\n",
        "---\n",
        "\n",
        "## üîé Academic Inference Function\n",
        "\n",
        "The `academic_chat()` function performs:\n",
        "\n",
        "1. **Prompt construction**\n",
        " - Combines instruction, system prompt, question, and answer placeholder\n",
        "2. **Tokenization**\n",
        " - Applies truncation to respect context limits\n",
        "3. **Controlled generation**\n",
        " - Beam search for better coherence\n",
        " - Repetition penalties to avoid loops\n",
        " - Deterministic decoding for reproducibility\n",
        "4. **Decoding**\n",
        " - Converts tokens back to clean text output\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Decoding Configuration Highlights\n",
        "\n",
        "- üîÄ `num_beams = 4` ‚Üí improves answer quality  \n",
        "- üîÅ `repetition_penalty = 1.3` ‚Üí reduces redundancy  \n",
        "- üö´ `no_repeat_ngram_size = 3` ‚Üí prevents tautology  \n",
        "- üéØ `do_sample = False` ‚Üí ensures deterministic outputs  \n",
        "\n",
        "These settings produce **clear, stable, and academically aligned responses**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Qualitative Testing\n",
        "\n",
        "Three test cases are included to demonstrate behavior:\n",
        "\n",
        "- ‚úÖ In-domain academic question  \n",
        "- ‚öñÔ∏è Out-of-domain technical question  \n",
        "- üö´ Casual, non-academic question  \n",
        "\n",
        "This validates that the model behaves as intended across different input types.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step matters\n",
        "\n",
        "- Confirms successful model restoration\n",
        "- Demonstrates controlled academic inference\n",
        "- Serves as the final evaluation and demo stage\n",
        "- Ready for submission, presentation, or viva\n",
        "\n",
        "---\n",
        "\n",
        "üìå *This cell represents the final, production-ready inference setup for the fine-tuned academic FLAN-T5 model.*\n"
      ],
      "metadata": {
        "id": "7OBBFTunGJd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell B: Load Model & Academic Inference\n",
        "# ==========================================\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "LOAD_DIR = \"/content/drive/MyDrive/flan_t5_academic_final/flan_t5_academic_final\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# üîπ Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(LOAD_DIR)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(LOAD_DIR).to(device)\n",
        "model.eval()\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an academic assistant.\\n\"\n",
        "    \"Answer questions related to Large Language Models and NLP in a formal, \"\n",
        "    \"textbook-style manner.\\n\"\n",
        "    \"If the question is from another technical domain, answer briefly and generically.\\n\"\n",
        "    \"If the question is casual or irrelevant, politely refuse.\\n\\n\"\n",
        ")\n",
        "\n",
        "def academic_chat(question, max_new_tokens=160):\n",
        "    prompt = (\n",
        "        f\"Instruction:\\n{SYSTEM_PROMPT}\"\n",
        "        f\"Question:\\n{question}\\n\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=4,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.3,\n",
        "            no_repeat_ngram_size=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# üîç Quick tests\n",
        "print(\"In-domain:\\n\", academic_chat(\"What is a Large Language Model?\"))\n",
        "print(\"\\nOut-of-domain:\\n\", academic_chat(\"What is cloud computing?\"))\n",
        "print(\"\\nCasual:\\n\", academic_chat(\"How is the weather today?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfyhrgS9ogsm",
        "outputId": "03ee8376-217c-4237-87a4-04e3e65c4cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-domain:\n",
            " Large Language Models are a set of language representations based on a large set of inputs. They can be used to model complex language patterns. NLP is a framework for evaluating large language models.\n",
            "\n",
            "Out-of-domain:\n",
            " Cloud computing is a system that stores large amounts of data on a single network. It is used to store large volumes of data across networks. Cloud computing relies on distributed architectures.\n",
            "\n",
            "Casual:\n",
            " rainy or cloudy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí¨ Interactive Academic Question‚ÄìAnswer Interface\n",
        "\n",
        "This cell provides an **interactive command-line interface** for querying the fine-tuned FLAN-T5 model in real time.  \n",
        "It allows users to explore the model‚Äôs behavior dynamically and evaluate its responses across different types of questions.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† User Interaction Flow\n",
        "\n",
        "The loop follows a simple and intuitive interaction pattern:\n",
        "\n",
        "1. The user enters a question\n",
        "2. The model generates an academic-style response\n",
        "3. The answer is displayed immediately\n",
        "4. The process repeats until the user exits\n",
        "\n",
        "---\n",
        "\n",
        "## üõë Exit & Input Handling\n",
        "\n",
        "- Typing **`exit`** cleanly terminates the session\n",
        "- Empty inputs are ignored with a helpful warning message\n",
        "\n",
        "These checks ensure a smooth and error-free user experience.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò Response Generation\n",
        "\n",
        "- All questions are routed through the `academic_chat()` function\n",
        "- The model responds using:\n",
        "  - formal textbook-style language\n",
        "  - controlled decoding\n",
        "  - deterministic generation\n",
        "\n",
        "This guarantees consistency with earlier inference behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Evaluation Use Cases\n",
        "\n",
        "This interactive loop is especially useful for:\n",
        "\n",
        "- Testing **in-domain academic questions**\n",
        "- Observing behavior on **out-of-domain technical queries**\n",
        "- Verifying **polite refusal** for casual or irrelevant prompts\n",
        "- Demonstrating the model live during presentations or viva exams\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why this step is important\n",
        "\n",
        "- Enables hands-on qualitative evaluation\n",
        "- Provides immediate feedback on model behavior\n",
        "- Serves as a user-friendly demo interface\n",
        "- Marks the final, end-to-end usage of the fine-tuned model\n",
        "\n",
        "---\n",
        "\n",
        "üìå *This interactive interface represents the final application layer built on top of the fine-tuned academic FLAN-T5 model.*\n"
      ],
      "metadata": {
        "id": "VUNxP4-cG0ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 12: Interactive Academic Q&A Loop\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìò Academic LLM Assistant\")\n",
        "print(\"Type your question and press Enter.\")\n",
        "print(\"Type 'exit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"üß† Question: \").strip()\n",
        "\n",
        "    if question.lower() == \"exit\":\n",
        "        print(\"\\nüëã Exiting. Session ended.\")\n",
        "        break\n",
        "\n",
        "    if len(question) == 0:\n",
        "        print(\"‚ö†Ô∏è Please enter a valid question.\\n\")\n",
        "        continue\n",
        "\n",
        "    answer = academic_chat(question)\n",
        "\n",
        "    print(\"\\nüìñ Answer:\\n\", answer)\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLWNk_h4o_wN",
        "outputId": "fb1e0d2d-f0dc-44ef-b55d-800cf3d23ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìò Academic LLM Assistant\n",
            "Type your question and press Enter.\n",
            "Type 'exit' to stop.\n",
            "\n",
            "üß† Question: Why are Large Language Models important in modern NLP?\n",
            "\n",
            "üìñ Answer:\n",
            " Large Language Models are used to model large language representations. They can be used to improve performance. NLP uses large language models to improve accuracy.\n",
            "------------------------------------------------------------\n",
            "üß† Question: How do Large Language Models differ from traditional language models?\n",
            "\n",
            "üìñ Answer:\n",
            " Large Language Models use large language representations to model complex language patterns. They are more efficient than traditional models. This reduces the need for retraining.\n",
            "------------------------------------------------------------\n",
            "üß† Question: What is fine-tuning in LLMs?\n",
            "\n",
            "üìñ Answer:\n",
            " Fine-tuning improves accuracy and consistency. It improves performance. NLP improves quality.\n",
            "------------------------------------------------------------\n",
            "üß† Question: What is tokenization in Large Language Models?\n",
            "\n",
            "üìñ Answer:\n",
            " tokenization identifies tokens in a large language model. This reduces the likelihood of false tokens. This improves model reliability.\n",
            "------------------------------------------------------------\n",
            "üß† Question: What is hallucination in Large Language Models?\n",
            "\n",
            "üìñ Answer:\n",
            " Hallucination occurs when a large language model fails to produce a single word or phrase. This causes the model to fail to learn a new word. It is common in large language models.\n",
            "------------------------------------------------------------\n",
            "üß† Question: what is cloud computing\n",
            "\n",
            "üìñ Answer:\n",
            " Cloud computing is a system that aggregates large language models and NLP data into a single model. It is often used as a central repository for large-language models. Cloud computing relies heavily on distributed architectures.\n",
            "------------------------------------------------------------\n",
            "üß† Question: how is the weather today\n",
            "\n",
            "üìñ Answer:\n",
            " rainy\n",
            "------------------------------------------------------------\n",
            "üß† Question: what are you trained on\n",
            "\n",
            "üìñ Answer:\n",
            " Large Language Models\n",
            "------------------------------------------------------------\n",
            "üß† Question: exit\n",
            "\n",
            "üëã Exiting. Session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample questions\n",
        "\n",
        "* What is a Large Language Model?\n",
        "\n",
        "* Why are Large Language Models important in modern NLP?\n",
        "\n",
        "* What is pretraining in Large Language Models?\n",
        "\n",
        "* What is fine-tuning in LLMs?\n",
        "\n",
        "* How do Large Language Models differ from traditional language models?\n",
        "\n",
        "* Why is attention important in transformer-based models?\n",
        "\n",
        "* What is tokenization in Large Language Models?\n",
        "\n",
        "* What is hallucination in Large Language Models?\n",
        "\n",
        "* Why is evaluation important for LLM systems?"
      ],
      "metadata": {
        "id": "i7gwAe0Tqp9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÅ Conclusion\n",
        "\n",
        "This project successfully demonstrated the **end-to-end fine-tuning of a Large Language Model** to behave as an **academic, textbook-style assistant**. Starting from dataset construction and validation, the workflow progressed through instruction formatting, safe tokenization, stable training, and controlled inference.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Key Achievements\n",
        "\n",
        "- üìò Curated and validated a **high-quality academic instruction dataset**\n",
        "- üî§ Implemented **safe tokenization** to prevent NaN loss and training collapse\n",
        "- üèãÔ∏è Fine-tuned **FLAN-T5-base** with a **numerically stable training loop**\n",
        "- üìâ Achieved smooth convergence with **no overfitting or instability**\n",
        "- üéì Enforced **formal academic response style** through prompt engineering\n",
        "- üíæ Preserved the trained model for reuse via **Google Drive integration**\n",
        "- üí¨ Built an **interactive academic Q&A interface** for real-time evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Observations & Insights\n",
        "\n",
        "- Instruction tuning significantly improves **response structure and tone**\n",
        "- Careful label handling is critical for **stable sequence-to-sequence training**\n",
        "- Prompt design plays a major role in **behavior control at inference time**\n",
        "- Domain-specific fine-tuning enhances accuracy at the cost of generality\n",
        "- Loss convergence is a useful indicator, but **qualitative evaluation is essential**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Limitations\n",
        "\n",
        "- The model is intentionally specialized for **LLM and NLP-related questions**\n",
        "- Casual or unrelated queries are not the primary focus\n",
        "- Broader generalization would require additional diverse training data\n",
        "\n",
        "These limitations are acceptable given the **academic objective** of the project.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Future Scope\n",
        "\n",
        "Potential extensions of this work include:\n",
        "\n",
        "- Expanding the dataset to cover additional academic domains\n",
        "- Adding explicit refusal examples for stronger safety behavior\n",
        "- Applying parameter-efficient fine-tuning methods such as **LoRA**\n",
        "- Deploying the model as a **web-based academic assistant**\n",
        "- Evaluating performance using standardized academic benchmarks\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Final Remarks\n",
        "\n",
        "This project highlights the importance of **data quality, training stability, and prompt control** in building reliable language models.  \n",
        "The resulting system serves as a **robust academic assistant**, suitable for educational use, demonstrations, and further research experimentation.\n",
        "\n",
        "---\n",
        "\n",
        "üìå *The notebook provides a complete, reproducible, and well-documented pipeline for academic instruction tuning of Large Language Models.*"
      ],
      "metadata": {
        "id": "9s9u1b1mHFeu"
      }
    }
  ]
}